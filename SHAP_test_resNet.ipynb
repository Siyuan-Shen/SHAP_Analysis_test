{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Net Construction\n",
    "\n",
    "ReLU_ACF = False\n",
    "Tanh_ACF = True\n",
    "GeLU_ACF = False\n",
    "Sigmoid_ACF = False\n",
    "\n",
    "def activation_function_table():\n",
    "    if ReLU_ACF == True:\n",
    "        return nn.ReLU()\n",
    "    elif Tanh_ACF == True:\n",
    "        return nn.Tanh()\n",
    "    elif GeLU_ACF == True:\n",
    "        return nn.GELU()\n",
    "    elif Sigmoid_ACF == True:\n",
    "        return nn.Sigmoid()\n",
    "\n",
    "activation_func = activation_function_table()\n",
    "\n",
    "\n",
    "def resnet_block_lookup_table(blocktype):\n",
    "    if blocktype == 'BasicBlock':\n",
    "        return BasicBlock\n",
    "    elif blocktype == 'Bottleneck':\n",
    "        return Bottleneck\n",
    "    else:\n",
    "        print(' Wrong Key Word! BasicBlock or Bottleneck only! ')\n",
    "        return None\n",
    "    \n",
    "\n",
    "class BasicBlock(nn.Module):  \n",
    "    \n",
    "    expansion = 1  \n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)  \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.tanh = activation_func\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = out + identity # out=F(X)+X\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):  \n",
    "    # Three convolutional layers, F(x) and X have different dimensions.\n",
    "    \n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None, groups=1, width_per_group=64):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        width = int(out_channel * (width_per_group / 64.)) * groups\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,kernel_size=1, stride=1, bias=False)  # squeeze channels\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel * self.expansion,kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)\n",
    "\n",
    "        self.Tanh = activation_func\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        identity = x\n",
    "       \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.Tanh(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.Tanh(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        # out=F(X)+X\n",
    "        out = out + identity\n",
    "        out = self.Tanh(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "  \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nchannel, # initial input channel\n",
    "                 block,  # block types\n",
    "                 blocks_num,  \n",
    "                 num_classes=1,  \n",
    "                 include_top=True, \n",
    "                 groups=1,\n",
    "                 width_per_group=64):\n",
    "\n",
    "        super(ResNet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.in_channel = 64  \n",
    "\n",
    "        self.groups = groups\n",
    "        self.width_per_group = width_per_group\n",
    "        self.actfunc = activation_func\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(nchannel, self.in_channel, kernel_size=7, stride=2,padding=3, bias=False)\n",
    "        #self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "\n",
    "        #self.tanh = nn.Tanh()\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        #self.layer0 = nn.Sequential(self.conv1,self.bn1,self.tanh,self.maxpool)\n",
    "        self.layer0 = nn.Sequential(nn.Conv2d(nchannel, self.in_channel, kernel_size=7, stride=2,padding=3, bias=False) #output size:6x6\n",
    "        #self.layer0 = nn.Sequential(nn.Conv2d(nchannel, self.in_channel, kernel_size=5, stride=1,padding=1, bias=False)\n",
    "        ,nn.BatchNorm2d(self.in_channel)\n",
    "        ,activation_func\n",
    "        ,nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) # output 4x4\n",
    "\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=1)\n",
    "        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=1)\n",
    "\n",
    "        if self.include_top: \n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  \n",
    "            \n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules(): \n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "   \n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channel * block.expansion))\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(block(self.in_channel,\n",
    "                            channel,\n",
    "                            downsample=downsample,\n",
    "                            stride=stride,\n",
    "                            groups=self.groups,\n",
    "                            width_per_group=self.width_per_group))\n",
    "\n",
    "        self.in_channel = channel * block.expansion # The input channel changed here!``\n",
    "        \n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel,\n",
    "                                channel,\n",
    "                                groups=self.groups,\n",
    "                                width_per_group=self.width_per_group))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.conv1(x)\n",
    "        #x = self.bn1(x)\n",
    "        #x = self.tanh(x)\n",
    "        #x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.include_top:  \n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            #x = self.actfunc(x)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = np.load('test_observation_data.npy')\n",
    "training_data = np.load('test_training_data.npy')\n",
    "X_train = training_data[0:100,:,:,:]\n",
    "y_train = label_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([100, 42, 11, 11])\n",
      "Epoch : 1/10, Iter : 1/10,  Loss: 0.6480\n",
      "Epoch : 1/10, Iter : 2/10,  Loss: 19.8370\n",
      "Epoch : 1/10, Iter : 3/10,  Loss: 11.3170\n",
      "Epoch : 1/10, Iter : 4/10,  Loss: 6.0826\n",
      "Epoch : 1/10, Iter : 5/10,  Loss: 5.1005\n",
      "Epoch : 1/10, Iter : 6/10,  Loss: 2.4360\n",
      "Epoch : 1/10, Iter : 7/10,  Loss: 4.1780\n",
      "Epoch : 1/10, Iter : 8/10,  Loss: 2.1278\n",
      "Epoch : 1/10, Iter : 9/10,  Loss: 1.1613\n",
      "Epoch : 1/10, Iter : 10/10,  Loss: 0.2980\n",
      "Epoch : 2/10, Iter : 1/10,  Loss: 0.2670\n",
      "Epoch : 2/10, Iter : 2/10,  Loss: 1.0585\n",
      "Epoch : 2/10, Iter : 3/10,  Loss: 0.8036\n",
      "Epoch : 2/10, Iter : 4/10,  Loss: 2.5073\n",
      "Epoch : 2/10, Iter : 5/10,  Loss: 1.5092\n",
      "Epoch : 2/10, Iter : 6/10,  Loss: 0.7102\n",
      "Epoch : 2/10, Iter : 7/10,  Loss: 0.3595\n",
      "Epoch : 2/10, Iter : 8/10,  Loss: 1.0772\n",
      "Epoch : 2/10, Iter : 9/10,  Loss: 0.6131\n",
      "Epoch : 2/10, Iter : 10/10,  Loss: 0.3145\n",
      "Epoch : 3/10, Iter : 1/10,  Loss: 0.8397\n",
      "Epoch : 3/10, Iter : 2/10,  Loss: 1.2470\n",
      "Epoch : 3/10, Iter : 3/10,  Loss: 0.5016\n",
      "Epoch : 3/10, Iter : 4/10,  Loss: 0.6183\n",
      "Epoch : 3/10, Iter : 5/10,  Loss: 0.2253\n",
      "Epoch : 3/10, Iter : 6/10,  Loss: 0.6399\n",
      "Epoch : 3/10, Iter : 7/10,  Loss: 0.2940\n",
      "Epoch : 3/10, Iter : 8/10,  Loss: 0.4231\n",
      "Epoch : 3/10, Iter : 9/10,  Loss: 0.4368\n",
      "Epoch : 3/10, Iter : 10/10,  Loss: 0.9821\n",
      "Epoch : 4/10, Iter : 1/10,  Loss: 0.4952\n",
      "Epoch : 4/10, Iter : 2/10,  Loss: 1.3665\n",
      "Epoch : 4/10, Iter : 3/10,  Loss: 0.3520\n",
      "Epoch : 4/10, Iter : 4/10,  Loss: 0.5978\n",
      "Epoch : 4/10, Iter : 5/10,  Loss: 0.5608\n",
      "Epoch : 4/10, Iter : 6/10,  Loss: 1.4097\n",
      "Epoch : 4/10, Iter : 7/10,  Loss: 0.3125\n",
      "Epoch : 4/10, Iter : 8/10,  Loss: 0.7280\n",
      "Epoch : 4/10, Iter : 9/10,  Loss: 0.4493\n",
      "Epoch : 4/10, Iter : 10/10,  Loss: 0.6163\n",
      "Epoch : 5/10, Iter : 1/10,  Loss: 0.9116\n",
      "Epoch : 5/10, Iter : 2/10,  Loss: 0.2901\n",
      "Epoch : 5/10, Iter : 3/10,  Loss: 0.2969\n",
      "Epoch : 5/10, Iter : 4/10,  Loss: 0.3229\n",
      "Epoch : 5/10, Iter : 5/10,  Loss: 0.1078\n",
      "Epoch : 5/10, Iter : 6/10,  Loss: 0.8630\n",
      "Epoch : 5/10, Iter : 7/10,  Loss: 0.6855\n",
      "Epoch : 5/10, Iter : 8/10,  Loss: 0.2615\n",
      "Epoch : 5/10, Iter : 9/10,  Loss: 1.1696\n",
      "Epoch : 5/10, Iter : 10/10,  Loss: 0.9442\n",
      "Epoch : 6/10, Iter : 1/10,  Loss: 0.1804\n",
      "Epoch : 6/10, Iter : 2/10,  Loss: 0.4615\n",
      "Epoch : 6/10, Iter : 3/10,  Loss: 1.1274\n",
      "Epoch : 6/10, Iter : 4/10,  Loss: 0.4628\n",
      "Epoch : 6/10, Iter : 5/10,  Loss: 1.1519\n",
      "Epoch : 6/10, Iter : 6/10,  Loss: 0.2746\n",
      "Epoch : 6/10, Iter : 7/10,  Loss: 0.9222\n",
      "Epoch : 6/10, Iter : 8/10,  Loss: 0.1900\n",
      "Epoch : 6/10, Iter : 9/10,  Loss: 0.0396\n",
      "Epoch : 6/10, Iter : 10/10,  Loss: 0.8007\n",
      "Epoch : 7/10, Iter : 1/10,  Loss: 0.2901\n",
      "Epoch : 7/10, Iter : 2/10,  Loss: 0.2008\n",
      "Epoch : 7/10, Iter : 3/10,  Loss: 0.2821\n",
      "Epoch : 7/10, Iter : 4/10,  Loss: 0.3579\n",
      "Epoch : 7/10, Iter : 5/10,  Loss: 0.9342\n",
      "Epoch : 7/10, Iter : 6/10,  Loss: 1.2347\n",
      "Epoch : 7/10, Iter : 7/10,  Loss: 0.1778\n",
      "Epoch : 7/10, Iter : 8/10,  Loss: 0.7014\n",
      "Epoch : 7/10, Iter : 9/10,  Loss: 0.2178\n",
      "Epoch : 7/10, Iter : 10/10,  Loss: 0.1556\n",
      "Epoch : 8/10, Iter : 1/10,  Loss: 0.1416\n",
      "Epoch : 8/10, Iter : 2/10,  Loss: 0.7273\n",
      "Epoch : 8/10, Iter : 3/10,  Loss: 0.3823\n",
      "Epoch : 8/10, Iter : 4/10,  Loss: 0.3386\n",
      "Epoch : 8/10, Iter : 5/10,  Loss: 0.4486\n",
      "Epoch : 8/10, Iter : 6/10,  Loss: 0.9641\n",
      "Epoch : 8/10, Iter : 7/10,  Loss: 0.2206\n",
      "Epoch : 8/10, Iter : 8/10,  Loss: 0.1710\n",
      "Epoch : 8/10, Iter : 9/10,  Loss: 0.3636\n",
      "Epoch : 8/10, Iter : 10/10,  Loss: 0.5424\n",
      "Epoch : 9/10, Iter : 1/10,  Loss: 0.4961\n",
      "Epoch : 9/10, Iter : 2/10,  Loss: 0.3663\n",
      "Epoch : 9/10, Iter : 3/10,  Loss: 0.3461\n",
      "Epoch : 9/10, Iter : 4/10,  Loss: 0.6316\n",
      "Epoch : 9/10, Iter : 5/10,  Loss: 0.4788\n",
      "Epoch : 9/10, Iter : 6/10,  Loss: 0.7515\n",
      "Epoch : 9/10, Iter : 7/10,  Loss: 0.8233\n",
      "Epoch : 9/10, Iter : 8/10,  Loss: 0.3069\n",
      "Epoch : 9/10, Iter : 9/10,  Loss: 0.5070\n",
      "Epoch : 9/10, Iter : 10/10,  Loss: 0.2443\n",
      "Epoch : 10/10, Iter : 1/10,  Loss: 0.2526\n",
      "Epoch : 10/10, Iter : 2/10,  Loss: 0.4952\n",
      "Epoch : 10/10, Iter : 3/10,  Loss: 0.0712\n",
      "Epoch : 10/10, Iter : 4/10,  Loss: 0.3341\n",
      "Epoch : 10/10, Iter : 5/10,  Loss: 0.3743\n",
      "Epoch : 10/10, Iter : 6/10,  Loss: 0.2053\n",
      "Epoch : 10/10, Iter : 7/10,  Loss: 1.8975\n",
      "Epoch : 10/10, Iter : 8/10,  Loss: 0.5225\n",
      "Epoch : 10/10, Iter : 9/10,  Loss: 0.2349\n",
      "Epoch : 10/10, Iter : 10/10,  Loss: 0.3691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Epoch   = 10\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):  # 'Characterizes a dataset for PyTorch'\n",
    "    '''\n",
    "    This class is for training datasets. It is used for the global datasets, which is continuous data.\n",
    "    '''\n",
    "    def __init__(self, traindata, truedata):  # 'Initialization' Data Loading\n",
    "        '''\n",
    "\n",
    "        :param traindata:\n",
    "            Training data.\n",
    "        :param truedata:\n",
    "            Ture data to learn.\n",
    "        :param beginyear:\n",
    "            The begin year.\n",
    "        :param endyear:\n",
    "            The end year.\n",
    "        :param nsite:\n",
    "            The number of sites. For example, for overall observation it is 10870.\n",
    "        '''\n",
    "        super(Dataset, self).__init__()\n",
    "        self.traindatasets = torch.Tensor(traindata)  # Read training data from npy file\n",
    "        self.truedatasets = torch.Tensor(truedata)\n",
    "        print(self.truedatasets.shape)\n",
    "        print(self.traindatasets.shape)\n",
    "        self.transforms = transform  # 转为tensor形式\n",
    "        self.shape = self.traindatasets.shape\n",
    "    def __getitem__(self, index):  # 'Generates one sample of data'\n",
    "        # Select sample\n",
    "        traindata = self.traindatasets[index, :, :]\n",
    "        truedata = self.truedatasets[index]\n",
    "        return traindata, truedata\n",
    "        # Load data and get label\n",
    "    def __len__(self):  # 'Denotes the total number of samples'\n",
    "        return self.traindatasets.shape[0]  # Return the total number of dataset\n",
    "\n",
    "def train(model, X_train, y_train,BATCH_SIZE, learning_rate):\n",
    "    train_loader = DataLoader(Dataset(X_train, y_train), BATCH_SIZE, shuffle=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9)\n",
    "    for epoch in range(Epoch):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            images = images.to(device)\n",
    "            labels = torch.squeeze(labels.type(torch.FloatTensor))\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(images) \n",
    "            outputs = torch.squeeze(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  \n",
    "            optimizer.step() \n",
    "            print('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f' % (epoch + 1, Epoch,\n",
    "                                                                    i + 1, len(X_train) // BATCH_SIZE,\n",
    "                                                                    loss.item()))\n",
    "    return \n",
    "\n",
    "device = torch.device('cpu')\n",
    "cnn_model = ResNet(nchannel=42, block=BasicBlock,blocks_num=[1,1,1,1]).to(device)\n",
    "train(cnn_model, X_train, y_train, Epoch, learning_rate=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m Data_to_Explain  \u001b[38;5;241m=\u001b[39m Data_to_Explain\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m CNNModel_Explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model\u001b[38;5;241m=\u001b[39mcnn_model,data\u001b[38;5;241m=\u001b[39mBack_Ground_Data)\n\u001b[0;32m----> 8\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mCNNModel_Explainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mData_to_Explain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/shap/explainers/_deep/__init__.py:159\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py:186\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[1;32m    185\u001b[0m feature_ind \u001b[38;5;241m=\u001b[39m model_output_ranks[j, i]\n\u001b[0;32m--> 186\u001b[0m sample_phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterim:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py:119\u001b[0m, in \u001b[0;36mPyTorchDeep.gradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[0;32m--> 119\u001b[0m         grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m             grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    409\u001b[0m         grad_outputs_\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    425\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/torch/utils/hooks.py:138\u001b[0m, in \u001b[0;36mBackwardHook._set_user_hook.<locals>.hook\u001b[0;34m(grad_input, _)\u001b[0m\n\u001b[1;32m    135\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pack_with_none(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_tensors_index, grad_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inputs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_hooks:\n\u001b[0;32m--> 138\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py:241\u001b[0m, in \u001b[0;36mdeeplift_grad\u001b[0;34m(module, grad_input, grad_output)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;129;01min\u001b[39;00m op_handler:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op_handler[module_type]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear_1d\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop_handler\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized nn.Module: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py:351\u001b[0m, in \u001b[0;36mnonlinear_1d\u001b[0;34m(module, grad_input, grad_output)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# handles numerical instabilities where delta_in is very small by\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# just taking the gradient in those cases\u001b[39;00m\n\u001b[1;32m    349\u001b[0m grads \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m grad_input]\n\u001b[1;32m    350\u001b[0m grads[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39mabs(delta_in\u001b[38;5;241m.\u001b[39mrepeat(dup0)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m, grad_input[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 351\u001b[0m                        \u001b[43mgrad_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta_in\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdup0\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(grads)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# SHAP Value Analysis\n",
    "cnn_model.eval()\n",
    "Back_Ground_Data = torch.Tensor(training_data[0:100,:,:,:])\n",
    "Data_to_Explain  = torch.Tensor(training_data[100:103,:,:,:])\n",
    "Back_Ground_Data = Back_Ground_Data.to(device)\n",
    "Data_to_Explain  = Data_to_Explain.to(device)\n",
    "CNNModel_Explainer = shap.DeepExplainer(model=cnn_model,data=Back_Ground_Data)\n",
    "shap_values = CNNModel_Explainer.shap_values(Data_to_Explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "Cell In[16], line 8\n",
    "      6 Data_to_Explain  = Data_to_Explain.to(device)\n",
    "      7 CNNModel_Explainer = shap.DeepExplainer(model=cnn_model,data=Back_Ground_Data)\n",
    "----> 8 shap_values = CNNModel_Explainer.shap_values(Data_to_Explain)\n",
    "\n",
    "File /opt/homebrew/anaconda3/lib/python3.10/site-packages/shap/explainers/_deep/__init__.py:159, in DeepExplainer.shap_values(self, X, ranked_outputs, output_rank_order, check_additivity)\n",
    "    115 def shap_values(self, X, ranked_outputs=None, output_rank_order='max', check_additivity=True):\n",
    "    116     \"\"\"Return approximate SHAP values for the model applied to the data given by X.\n",
    "    117 \n",
    "    118     Parameters\n",
    "   (...)\n",
    "    157 \n",
    "    158     \"\"\"\n",
    "--> 159     return self.explainer.shap_values(X, ranked_outputs, output_rank_order, check_additivity=check_additivity)\n",
    "\n",
    "File /opt/homebrew/anaconda3/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py:186, in PyTorchDeep.shap_values(self, X, ranked_outputs, output_rank_order, check_additivity)\n",
    "    184 # run attribution computation graph\n",
    "    185 feature_ind = model_output_ranks[j, i]\n",
    "--> 186 sample_phis = self.gradient(feature_ind, joint_x)\n",
    "    187 # assign the attributions to the right part of the output arrays\n",
    "    188 if self.interim:\n",
    "\n",
    "File /opt/homebrew/anaconda3/lib/python3.10/site-packages/shap/explainers/_deep/deep_pytorch.py:119, in PyTorchDeep.gradient(self, idx, inputs)\n",
    "...\n",
    "    350 grads[0] = torch.where(torch.abs(delta_in.repeat(dup0)) < 1e-6, grad_input[0],\n",
    "--> 351                        grad_output[0] * (delta_out / delta_in).repeat(dup0))\n",
    "    352 return tuple(grads)\n",
    "\n",
    "RuntimeError: The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 1\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
